{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manikanta5112/embeddings/blob/main/Faster_Embeddings_with_Optimum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the full benchmark report on [Optimum Benchmark x MTEB](https://github.com/huggingface/optimum-benchmark/tree/main/examples/fast-mteb) üìä\n",
        "CPU benchmarks are coming soon!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/huggingface/optimum-benchmark/main/examples/fast-mteb/artifacts/forward_latency_plot.png\" alt=\"Latency\" width=\"45%\"/>\n",
        "  <img src=\"https://raw.githubusercontent.com/huggingface/optimum-benchmark/main/examples/fast-mteb/artifacts/forward_throughput_plot.png\" alt=\"Latency\" width=\"45%\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "99e01NciY7Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is why your RAG system is slow üêå and unscalable üò±!\n",
        "\n",
        "All RAG implementations I have seen so far are embedding text using vanilla Pytorch (Sentence-Transformers) as a backend ü§¶.\n",
        "\n",
        "What are the consequences ‚ùì\n",
        "üìÑ With documents, this makes your vector database/index costs more compute than it should üí∏!\n",
        "üåê With web search, this makes your app slower and limited in terms of how many search results it can process üîé!\n",
        "\n",
        "How do we solve this ‚ùì\n",
        "- More throughput -> Better scalability!\n",
        "- Less latency -> Better user experience!\n",
        "Concretely, we use Optimum which provides direct support for ONNX export and inference with ONNX Runtime's advanced graph optimizations (no quality degradation üòé).\n",
        "\n",
        "Let's take Beijing Academy of Artificial Intelligence(BAAI)'s bge-base-en-v1.5, the number one base embedding model on the MTEB Leaderboard üèÜ, for a ride on Optimum üèéÔ∏è.\n",
        "\n",
        "With one CLI command I got 1 millisecond latency and 2000 samples per second throughput. Compared to vanilla Pytorch, this is 7x acceleration on both axes ü§Ø!\n",
        "\n",
        "üìí Notebook demonstrating how to use Optimum for faster sentence embeddings: https://lnkd.in/emt-2My5\n",
        "\n",
        "üìä Full benchmark configurations and report for reproduction:"
      ],
      "metadata": {
        "id": "gz35qhVKaXN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vJfvXCwUaXMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qbl2sEkMD1Z"
      },
      "outputs": [],
      "source": [
        "#@title We'll be using Optimum's OnnxRuntime support with `CUDAExecutionProvider` [because it's fast while also supporting dynamic shapes](https://github.com/huggingface/optimum-benchmark/tree/main/examples/fast-mteb#notes)\n",
        "\n",
        "!pip install optimum[onnxruntime-gpu]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [`optimum-cli`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization#optimizing-a-model-during-the-onnx-export) makes it extremely easy to export a model to ONNX and apply SOTA graph optimizations/fusions\n",
        "\n",
        "!optimum-cli export onnx \\\n",
        "  --model BAAI/bge-base-en-v1.5 \\\n",
        "  --task feature-extraction \\\n",
        "  --optimize O4 \\\n",
        "  --device cuda \\\n",
        "  bge_auto_opt_O4 # output folder"
      ],
      "metadata": {
        "id": "zEpRXKb1ReUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Based on the example given in [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5#using-huggingface-transformers)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/bge_auto_opt_O4')\n",
        "ort_model = ORTModelForFeatureExtraction.from_pretrained('/content/bge_auto_opt_O4', provide=\"CUDAExecutionProvider\")\n",
        "\n",
        "# Sentences we want sentence embeddings for\n",
        "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
        "\n",
        "# Tokenize sentences\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
        "# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = ort_model(**encoded_input)\n",
        "    # Perform pooling. In this case, cls pooling.\n",
        "    sentence_embeddings = model_output[0][:, 0]\n",
        "# normalize embeddings\n",
        "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
        "print(\"Sentence embeddings:\")\n",
        "print(sentence_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbRYCoGBR4yY",
        "outputId": "d3aea815-7e66-4373-8f55-d231d8a5ebce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence embeddings:\n",
            "tensor([[ 0.0251,  0.0052,  0.0221,  ...,  0.0092, -0.0090, -0.0150],\n",
            "        [-0.0125,  0.0129,  0.0137,  ...,  0.0215,  0.0258,  0.0107]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KzGEA4YpT8XP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}